{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maWkMPK634t8"
      },
      "outputs": [],
      "source": [
        "#check path # for dev opnly\n",
        "!pwd\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dVy6MPa3rHj"
      },
      "source": [
        "<h1><b><center>Process 1/2: Text To Art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQzcBoRR3qxc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <center> Setup block\n",
        "#@markdown <center>Run Once\n",
        "\n",
        "\n",
        "print(\" starting 2. Downloading and Installation of libraries\")\n",
        "\n",
        "print(\"Downloading CLIP...\")\n",
        "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
        " \n",
        "print(\"Installing Python Libraries for AI...\")\n",
        "!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n",
        "!pip install kornia                                       &> /dev/null\n",
        "!pip install einops                                       &> /dev/null\n",
        "!pip install wget                                         &> /dev/null\n",
        " \n",
        "print(\"Installing libraries for handling metadata...\")\n",
        "!pip install stegano                                      &> /dev/null\n",
        "!apt install exempi                                       &> /dev/null\n",
        "!pip install python-xmp-toolkit                           &> /dev/null\n",
        "!pip install imgtag                                       &> /dev/null\n",
        "!pip install pillow==7.1.2                                &> /dev/null\n",
        " \n",
        "print(\"Installing Python libraries for creating videos ...\")\n",
        "!pip install imageio-ffmpeg                               &> /dev/null\n",
        "!mkdir steps\n",
        "print(\"Step 2. done\")\n",
        "\n",
        "\n",
        "print(\"3. models to download\")\n",
        "!curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n",
        "!curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n",
        "print(\"Step 3. done\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"4. Loading Libraries and definitions\")\n",
        " \n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        " \n",
        "sys.path.append('./taming-transformers')\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "from imgtag import ImgTag    # metadatos \n",
        "from libxmp import *         # metadatos\n",
        "import libxmp                # metadatos\n",
        "from stegano import lsb\n",
        "import json\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        " \n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        " \n",
        " \n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        " \n",
        " \n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        " \n",
        " \n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        " \n",
        "    input = input.view([n * c, 1, h, w])\n",
        " \n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        " \n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        " \n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        " \n",
        " \n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        " \n",
        " \n",
        "replace_grad = ReplaceGrad.apply\n",
        " \n",
        " \n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        " \n",
        " \n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        " \n",
        " \n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        " \n",
        " \n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        " \n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        " \n",
        " \n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        " \n",
        " \n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        " \n",
        " \n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        print(config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        " \n",
        " \n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "def download_img(img_url):\n",
        "    try:\n",
        "        return wget.download(img_url,out=\"input.jpg\") # type: ignore\n",
        "    except:\n",
        "        return\n",
        "\n",
        "print(\"Step 4. done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT6Z0NGj3qhp"
      },
      "outputs": [],
      "source": [
        "#@title <center>Action Block\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "print(\"5. Give your text prompts and settings here\")\n",
        "\n",
        "\n",
        "#@markdown <br> <center> <b>INSTRUCTION: Fill the form then run this Block\n",
        "\n",
        "#@markdown <br><center><p><b>Enter your text prompt to generate images</b>\n",
        "#@markdown <br> (separated with | )\n",
        "#@markdown <br> You can also set weights with a <b>: (colon)</b>\n",
        "#@markdown <br> <p><i>ice forest :8 | trending on Artstation :9\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_text_for_image_generation = \"fan on planet | artstation\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <br><br><center><p>Set Width and Height of output images\n",
        "\n",
        "width_of_output_image =  300#@param {type:\"number\"}\n",
        "height_of_output_image =  150#@param {type:\"number\"}\n",
        "\n",
        "#@markdown <br><br><center>\n",
        "#<p>Select only the models that are downloaded in step 3\n",
        "model_to_use = \"vqgan_imagenet_f16_16384\" # @ param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"wikiart_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\", \"ade20k\", \"ffhq\", \"celebahq\", \"gumbel_8192\"]\n",
        "\n",
        "#@markdown <br><br><center><p>Select an image to enable the Algorithm to start creating. Transparent png images with few dots work best.\n",
        "initial_image = \"picture.png\" #@param {type:\"string\"}\n",
        "if initial_image==\"picture.png\" or \"-1\" or \"0\":\n",
        "  !wget -O picture.png https://www.dropbox.com/s/f75ts4p736s9560/sample_png_4.png?dl=0\n",
        "\n",
        "\n",
        "#@markdown <center><p><br><br>Steer your generated image towards this image\n",
        "\n",
        "target_images = \"None\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <br><br><center><p>-1 for random value<br>\n",
        "#@markdown Giving the same value (from 1 to 65536) every time, with same parameters will produce similar images \n",
        "seed = 20#@param {type:\"number\"}\n",
        "\n",
        "#@markdown <br><br><center><p>-1 for infinite output.\n",
        "#@markdown <br>any other value (recommended 400) >0 is valid\n",
        "#@markdown <br>Generally, higher this value, better the images \n",
        "\n",
        "max_iterations =  105#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_images = \"\"\n",
        "\n",
        "\n",
        "#simplified:\n",
        "textos = input_text_for_image_generation\n",
        "\n",
        "ancho=width_of_output_image\n",
        "alto = height_of_output_image\n",
        "modelo = model_to_use\n",
        "interval_images = 1\n",
        "\n",
        "\n",
        "download_step=100\n",
        "\n",
        "nombres_modelos={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n",
        "                 \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\", \"ade20k\":\"ADE20K\", \"ffhq\":\"FFHQ\", \"celebahq\":\"CelebA-HQ\", \"gumbel_8192\": \"Gumbel 8192\"}\n",
        "nombre_modelo = nombres_modelos[modelo]     \n",
        "\n",
        "if modelo == \"gumbel_8192\":\n",
        "    is_gumbel = True\n",
        "else:\n",
        "    is_gumbel = False\n",
        "\n",
        "if seed == -1:\n",
        "    seed = None\n",
        "if initial_image == \"None\":\n",
        "    initial_image = None\n",
        "elif initial_image and initial_image.lower().startswith(\"http\"):\n",
        "    initial_image = download_img(initial_image)\n",
        "\n",
        "\n",
        "if target_images == \"None\" or not target_images:\n",
        "    target_images = []\n",
        "else:\n",
        "    target_images = target_images.split(\"|\")\n",
        "    target_images = [image.strip() for image in target_images]\n",
        "\n",
        "if initial_image or target_images != []:\n",
        "    input_images = True\n",
        "\n",
        "textos = [frase.strip() for frase in textos.split(\"|\")]\n",
        "if textos == ['']:\n",
        "    textos = []\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompts=textos,\n",
        "    image_prompts=target_images,\n",
        "    noise_prompt_seeds=[],\n",
        "    noise_prompt_weights=[],\n",
        "    size=[ancho, alto],\n",
        "    init_image=initial_image,\n",
        "    init_weight=0.,\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=f'{modelo}.yaml',\n",
        "    vqgan_checkpoint=f'{modelo}.ckpt',\n",
        "    step_size=0.1,\n",
        "    cutn=64,\n",
        "    cut_pow=1.,\n",
        "    display_freq=interval_images,\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "print(\"5. done\")\n",
        "\n",
        "print(\"6. [Final step] Do the execution...\")\n",
        "#@markdown <br>\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "last_i=-1\n",
        "\n",
        "if textos:\n",
        "    print('Using texts:', textos)\n",
        "if target_images:\n",
        "    print('Using image prompts:', target_images)\n",
        "if args.seed is None:\n",
        "    seed = torch.seed()\n",
        "else:\n",
        "    seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "print('Using seed:', seed)\n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "if is_gumbel:\n",
        "    e_dim = model.quantize.embedding_dim\n",
        "else:\n",
        "    e_dim = model.quantize.e_dim\n",
        "\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "if is_gumbel:\n",
        "    n_toks = model.quantize.n_embed\n",
        "else:\n",
        "    n_toks = model.quantize.n_e\n",
        "\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "if is_gumbel:\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "else:\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "if args.init_image:\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\n",
        "    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "    z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "else:\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "    if is_gumbel:\n",
        "        z = one_hot @ model.quantize.embed.weight\n",
        "    else:\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "z_orig = z.clone()\n",
        "z.requires_grad_(True)\n",
        "opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "pMs = []\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "for prompt in args.image_prompts:\n",
        "    path, weight, stop = parse_prompt(prompt)\n",
        "    img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "    embed = perceptor.encode_image(normalize(batch)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "def synth(z):\n",
        "    if is_gumbel:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "    else:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "    \n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "def add_xmp_data(nombrefichero):\n",
        "    imagen = ImgTag(filename=nombrefichero)\n",
        "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    if args.prompts:\n",
        "        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    else:\n",
        "        imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', nombre_modelo, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'input_images',str(input_images) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    #for frases in args.prompts:\n",
        "    #    imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'Prompt' ,frases, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "    imagen.close()\n",
        "\n",
        "def add_stegano_data(filename):\n",
        "    data = {\n",
        "        \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
        "        \"notebook\": \"VQGAN+CLIP\",\n",
        "        \"i\": i,\n",
        "        \"model\": nombre_modelo,\n",
        "        \"seed\": str(seed),\n",
        "        \"input_images\": input_images\n",
        "    }\n",
        "    lsb.hide(filename, json.dumps(data)).save(filename)\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    global last_i,download_step\n",
        "    last_i=i\n",
        "\n",
        "    \n",
        "\n",
        "    if i==download_step:\n",
        "      download_step=download_step+100\n",
        "      try:\n",
        "        filename = f\"steps/{last_i:04}.png\"\n",
        "        files.download(filename)\n",
        "        print(\"downloading\"+filename)\n",
        "      except Exception as e:\n",
        "        print(\"Exc: \"+str(e))\n",
        "\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'\\n\\n\\nPrompt: {textos}\\nIteration number: {i}/{max_iterations}')\n",
        "    #\\nloss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z)\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "    add_stegano_data('progress.png')\n",
        "    add_xmp_data('progress.png')\n",
        "    display.display(display.Image('progress.png'))\n",
        "\n",
        "def ascend_txt():\n",
        "    global i\n",
        "    out = synth(z)\n",
        "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    filename = f\"steps/{i:04}.png\"\n",
        "    imageio.imwrite(filename, np.array(img))\n",
        "    add_stegano_data(filename)\n",
        "    add_xmp_data(filename)\n",
        "    return result\n",
        "\n",
        "def train(i):\n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    loss = sum(lossAll)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    with torch.no_grad():\n",
        "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while True:\n",
        "            train(i)\n",
        "            if i == max_iterations:\n",
        "                break\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "print(\"end of execution block\")\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "#creating name of file from prompts\n",
        "op_file_name = re.sub(r\"\\|\",\"_\",input_text_for_image_generation)\n",
        "op_file_name = re.sub(r\"\\s\", \"_\", op_file_name)\n",
        "op_file_name = re.sub(r\"\\__\", \"_\", op_file_name)\n",
        "op_file_name = re.sub(r\":\", \"\", op_file_name)\n",
        "\n",
        "#@markdown <br> <center> <b> Picture output settings\n",
        "\n",
        "#@markdown <p>Typically you'd skip the first few frames and start from say frame 10 or 20 if you've set a total iteration of 400\n",
        "#@markdown <p>To start from the beginning keep it at 0\n",
        "#This is the frame where the video will start\n",
        "no_of_frames_to_skip= 0 #@param {type:\"number\"}\n",
        "starting_frame = no_of_frames_to_skip\n",
        "\n",
        "#@markdown <p>\n",
        "no_of_images_to_download = 10#@param {type:\"number\"}\n",
        "#no_of_images_to_download=no_of_images_to_download-1\n",
        "\n",
        "last_frame=last_i\n",
        "step = int(last_frame/no_of_images_to_download)\n",
        "print(f\"starting_frame {starting_frame}\")\n",
        "print(f\"last_frame : {last_frame}\")\n",
        "print(f\"step : {step}\")\n",
        "\n",
        "filenames_to_download=\"\"\n",
        "for i in range ((starting_frame+step),last_frame+1,step):\n",
        "    filename = f\"steps/{i:04}.png\"\n",
        "    #files.download(filename)\n",
        "    filenames_to_download+=filename+\" \"\n",
        "\n",
        "import os\n",
        "try:\n",
        "  print(filenames_to_download)\n",
        "  zip_filename=op_file_name+\".zip\"\n",
        "  if os.path.exists(zip_filename):\n",
        "    os.remove(zip_filename)\n",
        "  os.system(f\"zip {zip_filename} {filenames_to_download}\")\n",
        "  #files.download(zip_filename)\n",
        "except Exception as e:\n",
        "  print(\"Error while zipping\")\n",
        "\n",
        "\n",
        "#@markdown <p> <br><br><br>\n",
        "#@markdown <br> <center> <b> Video output settings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "init_frame = 1 #This is the frame where the video will start\n",
        "\n",
        "last_frame = last_i #You can change i to the number of the last frame you want to generate. \n",
        "#It will raise an error if that number of frames does not exist.\n",
        "\n",
        "min_fps = 1#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "max_fps = 30#@param {type:\"number\"}\n",
        "\n",
        "total_frames = last_frame-init_frame\n",
        "\n",
        "\n",
        "length_of_video_in_secs = 15 #@param {type:\"number\"}\n",
        "#Desired time of the video in seconds\n",
        "length=length_of_video_in_secs\n",
        "\n",
        "frames = []\n",
        "tqdm.write('Generating video....')\n",
        "for i in range(0,last_i): #\n",
        "    filename = f\"steps/{i:04}.png\"\n",
        "    frames.append(Image.open(filename))\n",
        "\n",
        "#fps = last_frame/10\n",
        "fps = np.clip(total_frames/length,min_fps,max_fps)\n",
        "\n",
        "\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "\n",
        "op_vid_name=op_file_name+\".mp4\"\n",
        "\n",
        "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', op_vid_name], stdin=PIPE)\n",
        "for im in tqdm(frames):\n",
        "    im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "\n",
        "print(\"The video is now being compressed, wait...\")\n",
        "p.wait()\n",
        "print(\"video is ready to download\")\n",
        "\n",
        "from google.colab import files\n",
        "#files.download(op_vid_name)\n",
        "print(\"Downloading\")\n",
        "print(\"ALL DONE\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODllfLVB4xs4"
      },
      "source": [
        "___________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMexkHmO4xcH"
      },
      "source": [
        "_______________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXnnN6Qi3g_O"
      },
      "source": [
        "<h1><b><center>Process 2/2: Upscaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khTg5xZb2xd-"
      },
      "source": [
        "<h1>Init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%cd home_of_project"
      ],
      "metadata": {
        "id": "en2DvzU33WGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10MNe7PUvb4B"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pwd\n",
        "\n",
        "\n",
        "!pip3 install virtualenv\n",
        "!virtualenv newtestenv\n",
        "!source /content/newtestenv/bin/activate; pip3 list\n",
        "\n",
        "\n",
        "%bookmark home_of_project /content\n",
        "\n",
        "\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!source /content/newtestenv/bin/activate; git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "\n",
        "# Set up the environment\n",
        "!source /content/newtestenv/bin/activate;pip install basicsr\n",
        "!source /content/newtestenv/bin/activate;pip install facexlib\n",
        "!source /content/newtestenv/bin/activate;pip install gfpgan\n",
        "!source /content/newtestenv/bin/activate;pip install -r requirements.txt\n",
        "!source /content/newtestenv/bin/activate;python setup.py develop\n",
        "# Download the pre-trained model\n",
        "!source /content/newtestenv/bin/activate; wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3zfBDxG2ty7"
      },
      "source": [
        "<center> <h1><b>Upload"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#needed!!\n",
        "%bookmark home_of_project /content\n",
        "!pwd\n",
        "%bookmark -l\n",
        "%cd home_of_project\n"
      ],
      "metadata": {
        "id": "1u-iLEEoOSDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd home_of_project \n",
        "!pwd\n",
        "\n",
        "import shutil\n",
        "upload_folder = 'Real-ESRGAN/upload'\n",
        "result_folder = 'Real-ESRGAN/results'\n",
        "#copy the last picture\n",
        "src_fname=f\"{last_i:04}.png\"\n",
        "src_fname_and_path=f\"steps/{last_i:04}.png\"\n",
        "dst_path=os.path.join(upload_folder, src_fname)\n",
        "print(src_fname_and_path)\n",
        "print(dst_path)\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)\n",
        "\n",
        "print(f'copy {src_fname_and_path} to {upload_folder}')\n",
        "shutil.copy(src_fname_and_path, upload_folder)"
      ],
      "metadata": {
        "id": "an-DSbhrMf1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgQs9l6k2XWt"
      },
      "source": [
        "<center> <h1><b>Run Now"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set enlarging factor\n",
        "\n",
        "#needed!!\n",
        "print(op_file_name)\n",
        "!pwd\n",
        "%cd Real-ESRGAN/\n",
        "scale = 3 #@param {type:\"slider\", min:0, max:10, step:0.5}\n"
      ],
      "metadata": {
        "id": "U0rqHHXJPJzp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fBGuTeWxt22"
      },
      "outputs": [],
      "source": [
        "!source /content/newtestenv/bin/activate; python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale scale --face_enhance\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "uu6oYfmPQLuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download a zipped copy of the last frame \n",
        "# # @markdown Run this cell to connect to your gdrive and copy your results to the foldername you set here\n",
        "zip_filename_enhanced = op_file_name+\"_enhanced.zip\"\n",
        "if os.path.exists(zip_filename_enhanced):\n",
        "  os.remove(zip_filename_enhanced)\n",
        "os.system(f\"zip -r -j {zip_filename_enhanced} results/*\")\n",
        "files.download(zip_filename_enhanced)"
      ],
      "metadata": {
        "id": "yFNo3Bx28eEH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title If you give permission, <br> you can also copy the video and zip results of the selected photos and the Enhanced image. \n",
        "#@markdown Run this cell to connect to your gdrive and copy your results to the foldername you set here\n",
        "\n",
        "drive_folder_name = '0ColabOutputs' #@param {type:\"string\"}\n",
        "os.mkdir(f\"gdrive/MyDrive/{drive_folder_name}/\")\n",
        "!pwd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/Real-ESRGAN/gdrive')\n",
        "\n",
        "zip_filename_enhanced = op_file_name+\"_enhanced.zip\"\n",
        "shutil.copy(zip_filename_enhanced, f\"gdrive/MyDrive/{drive_folder_name}/\") # image enhancer\n",
        "\n",
        "%cd home_of_project\n",
        "shutil.copy(op_vid_name, f\"Real-ESRGAN/gdrive/MyDrive/{drive_folder_name}/\")\n",
        "shutil.copy(zip_filename,f\"Real-ESRGAN/gdrive/MyDrive/{drive_folder_name}/\")\n"
      ],
      "metadata": {
        "id": "PoNVrUMOvi3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g05fzbTm2kgq"
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text to art. step by step text to art then enhamcement.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}